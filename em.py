from music21 import converter, instrument, note, chord, stream, pitch
import glob
import sys
import numpy as np
import os
import mido
from sklearn import mixture
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

def read_midi(filename, size_motif):
    midi = converter.parse(filename)
    notes_i = []

    print("Parsing %s" % filename)

    notes_to_parse = None

    try: # file has instrument parts
        notes_to_parse = midi[0].recurse()
    except: # file has notes in a flat structure
        notes_to_parse = midi.flat.notes

    for element in notes_to_parse:
        if isinstance(element, note.Note):
            #notes_i.append(str(element.pitch))
            # Append the midi value
            notes_i.append(element.pitch.midi)
        elif isinstance(element, chord.Chord):
            notes_i.append(element.pitches[-1].midi) # take the note with the highest octave? This is a modification
        
    # standardize to snippets of length 5
    desired_length = len(notes_i) - (len(notes_i) % size_motif) 

    notes = notes_i[0:desired_length]
    return notes

def extract_snippet(notes, size):
    cut = len(notes)%size
    # cut off values from end of trajectory to match sizes
    if cut > 0:
        notes = notes[:-cut]
    snippets = np.split(notes, len(notes)/size)
    return snippets


def read_trajs(traj_dir, size_motif):
    em_set = []
    mapper = {}
    for i in range(100):
        traj_name = ''
        if i<10:
            #traj_e = np.loadtxt(traj_dir+"/e_000{}.txt".format(i))
            traj_g = np.loadtxt(traj_dir+"/g_000{}.txt".format(i))
            traj_name = '_000{}.txt'.format(i)
        else:
            #traj_e = np.loadtxt(traj_dir+"/e_00{}.txt".format(i))
            traj_g = np.loadtxt(traj_dir+"/g_00{}.txt".format(i))
            traj_name = '_00{}.txt'.format(i)
        
        #snippets_e = extract_snippet(traj_e, size_motif)
        snippets_g = extract_snippet(traj_g, size_motif)
    
        length = len(em_set)
        num_motifs = len(snippets_g)
        #mapper['e'+traj_name] = [length, length+num_motifs]
        #mapper['g'+traj_name] = [length+num_motifs, length+2*num_motifs]
        mapper['g'+traj_name] = [length, length+num_motifs]

        #em_set.extend(snippets_e)
        em_set.extend(snippets_g)

    em_set = np.array(em_set)
    return em_set, mapper

def run_em(em_set, k):
    " Run GMM with EM algorithm on set and gradient of set "
    print("yo")

    # Compute gradients of each snippet
    em_set_gradient = [np.gradient(item) for item in em_set]
    em_set_gradient = np.array(em_set_gradient)

    # Construct combination of raw em set and gradient em set
    em_gr_plus_raw = (1*em_set_gradient + 3*em_set)/4

    # Construct and fit a Gaussian Mixture model, then predict
    cv_type = 'full'
    gmm = mixture.GaussianMixture(n_components=k, covariance_type=cv_type)
    gmm.fit(em_gr_plus_raw)
    labels = gmm.predict(em_gr_plus_raw)
  
    # Compute pitch means for each k
    means = []
    for i in range(k):
        #means.append(np.sum(labels==i))
        tot = 0
        for j in labels:
            if j == i:
                tot += em_set[j]
        tot /= np.sum(labels==i)
        means.append(tot)
    means = np.array(means)

    return labels, means

def distance_loss(x, y):
    return np.linalg.norm(x-y, ord=2)


def gradient_loss(x, y):
    grad_diff = np.gradient(x) - np.gradient(y)
    return np.linalg.norm(grad_diff, ord=2)

def curve_loss(x, y):
    curve_diff = np.gradient(np.gradient(x)) - np.gradient(np.gradient(y))
    return np.linalg.norm(curve_diff, ord=2)

def match_means_to_motifs(means, song):
    matches = []
    for snippet in means:
        losses = []
        # Calculate losses for this snippet compared to each motif
        for motif in song:
            # TODO: weight parameters?
            A = 10
            B = 2
            C = 1
            loss = A*distance_loss(snippet, motif) + B*gradient_loss(snippet, motif) + C*curve_loss(snippet, motif)
            losses.append(loss)
        match = np.argmin(losses)
        print(match)
        matches.append(song[match])
    matches = np.array(matches)
    return matches

def create_midi(melody, filename):
    """ create a midi file from the notes """

    offset = 0
    output_notes = []

    # create note and chord objects based on the values generated by the model
    for pattern in melody:
        # pattern is a note
        try:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)

            # increase offset each iteration so that notes do not stack
            offset += 0.5
        except:
            print('Exception thrown, note not appended')

    midi_stream = stream.Stream(output_notes)

    filename = filename.replace('songs/','')
    midi_stream.write('midi', fp='simplified_songs/'+filename[:-4]+'_simple.mid')

def create_midi_from_notes(notes, filename, on=256, off=0, vel_on=90, vel_off=0):
    mid = mido.MidiFile()
    track = mido.MidiTrack()
    mid.tracks.append(track)
#    track.append(mido.Message('program_change', program=12, time=0))

    for note in notes:
        note = int(note)
        track.append(mido.Message('note_on', note=note, velocity=vel_on, time=off))
        track.append(mido.Message('note_off', note=note, velocity=vel_off, time=on))

    mid.save('em_samples_ground/'+filename+'.mid')


def write_songs(matches, mapper, labels):
    # Concatonate melodies for each trajectory, then write to a midi file
    for filename, idx in mapper.items():
        sample = []
        for i in range(idx[0], idx[1]):
            sample.extend(matches[labels[i]])

        create_midi_from_notes(sample, filename[:-4])


def plot_traj_song(matches, mapper, labels, em_set, size_motif):
    diff = 0
    for filename, idx in mapper.items():
        traj = []
        sample = []
        for i in range(idx[0], idx[1]):
            sample.extend(matches[labels[i]])
            traj.extend(em_set[i])
            diff += distance_loss(np.array(sample), np.array(traj))

        len_sample = (idx[1]-idx[0])*size_motif
        x = np.arange(0, len_sample)

        plt.clf()
        plt.figure(figsize=(20, 5))
        plt.plot(x, traj, '-_b', x, sample, '-_m')
        plt.legend(['original trajectory', 'generated notes'])
        plt.ylim(50, 100)
        plt.xlabel('time')
        plt.ylabel('pitch')
        plt.savefig('em_samples_ground_plots/'+filename[:-4]+'.pdf')

    diff /= len(mapper)
    return diff


def compute_diff(matches, mapper, labels, em_set, size_motif):
    for filename, idx in mapper.items():
        traj = []
        sample = []
        for i in range(idx[0], idx[1]):
            sample.extend(matches[labels[i]])
            traj.extend(em_set[i])

        len_sample = (idx[1]-idx[0])*size_motif
     


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print('Usage: python generate_em.py [traj dir] [song midi filename]')    

    traj_dir = sys.argv[1]
    midi_file = sys.argv[2]

    size_motif = 7
    # Read in midi file
    song = read_midi(midi_file, size_motif)
    print(song)

    k = int(len(song)/size_motif)
    song = np.split(np.array(song), k)
    song = np.array(song)
    print(song)
    print(song.shape)
    
    em_set, mapper = read_trajs(traj_dir, size_motif)    
    labels, means = run_em(em_set, k)
    print(means)
    print(means.shape)
    print(np.min(means))
    print(np.max(means))
    print(np.max(song))
    print(np.min(song))
    # matches are the music snippets in the same order as designated by labels
    matches = match_means_to_motifs(means, song)
    write_songs(matches, mapper, labels)

    print(em_set.shape)
    avg_diff = plot_traj_song(matches, mapper, labels, em_set, size_motif) 

    print("Average L2 diff between trajectory and sample: {}".format(avg_diff))











